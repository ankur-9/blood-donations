{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thanks to   \n",
    "\n",
    "* Emanuele Olivetti   \n",
    "https://github.com/emanuele/kaggle_pbr/blob/master/blend.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import random, sys, os, re\n",
    "\n",
    "from sklearn.ensemble         import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model     import LogisticRegression\n",
    "\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.grid_search      import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_predict, permutation_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED   = 97\n",
    "scale  = False \n",
    "minmax = False\n",
    "norm   = False\n",
    "nointercept = True\n",
    "engineering = True\n",
    "\n",
    "N_CLASSES = 2\n",
    "\n",
    "submission_filename = \"../submissions/submission_blending_ensemble.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from load_blood_data import load_blood_data\n",
    "\n",
    "y_train, X_train = load_blood_data(train=True, SEED   = SEED, \n",
    "                                               scale  = scale,\n",
    "                                               minmax = minmax,\n",
    "                                               norm   = norm,\n",
    "                                               nointercept = nointercept,\n",
    "                                               engineering = engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_blood_data import load_blood_data\n",
    "\n",
    "X_test, IDs = load_blood_data(train=False, SEED   = SEED, \n",
    "                                           scale  = scale,\n",
    "                                           minmax = minmax,\n",
    "                                           norm   = norm,\n",
    "                                           nointercept = nointercept,\n",
    "                                           engineering = engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StatifiedCV = StratifiedKFold(y            = y_train, \n",
    "                              n_folds      = 10, \n",
    "                              shuffle      = True, \n",
    "                              random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets for blending.\n",
      "\n",
      " 0, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      " 1, RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      " 2, ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      " 3, ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
      "           max_depth=7, max_features=None, max_leaf_nodes=None,\n",
      "           min_samples_leaf=1, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      " 4, GradientBoostingClassifier(init=None, learning_rate=0.05, loss='deviance',\n",
      "              max_depth=6, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "              presort='auto', random_state=None, subsample=0.5, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "\n",
      " 5, GradientBoostingClassifier(init=None, learning_rate=0.15, loss='exponential',\n",
      "              max_depth=1, max_features=None, max_leaf_nodes=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=175,\n",
      "              presort='auto', random_state=None, subsample=0.75, verbose=0,\n",
      "              warm_start=False)\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "CPU times: user 9.27 s, sys: 1.3 s, total: 10.6 s\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "X_train = X_train.values.astype(np.float32)\n",
    "X_test  = X_test.values.astype(np.float32)\n",
    "\n",
    "skf = list(StatifiedCV)\n",
    "\n",
    "# popular non-linear choices\n",
    "# GBM, RF, XT + KNN, NN \n",
    "\n",
    "clfs = [RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=100,   n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=10,    n_jobs=-1, criterion='entropy', max_depth=7, max_features=None),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.50, max_depth=6, n_estimators=50),\n",
    "        GradientBoostingClassifier(learning_rate=0.15, subsample=0.75, max_depth=1, n_estimators=175,\n",
    "                                   loss='exponential')\n",
    "       ]\n",
    "\n",
    "print \"Creating train and test sets for blending.\"\n",
    "\n",
    "dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "dataset_blend_test  = np.zeros((X_test.shape[0],  len(clfs)))\n",
    "\n",
    "for j, clf in enumerate(clfs):\n",
    "    print(\"\\n {}, {}\".format(j, clf))\n",
    "    dataset_blend_test_j = np.zeros((X_test.shape[0], len(skf)))\n",
    "    \n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Fold\", i\n",
    "        X_b_train = X_train[train]\n",
    "        y_b_train = y_train[train]\n",
    "        X_b_test  = X_train[test]\n",
    "        y_b_test  = y_train[test]\n",
    "        \n",
    "        clf.fit(X_b_train, y_b_train)\n",
    "        y_submission = clf.predict_proba(X_b_test)[:,1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i]   = clf.predict_proba(X_test)[:,1]\n",
    "        \n",
    "    dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending.\n",
      "Linear stretch of predictions to [0,1]\n",
      "\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 1.14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print \"Blending.\"\n",
    "clf = LogisticRegression()\n",
    "clf.fit(dataset_blend_train, y_train)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:,1]\n",
    "\n",
    "print \"Linear stretch of predictions to [0,1]\\n\"\n",
    "y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70195683  0.17863846  0.09775343  0.26805966  0.78357102  1.\n",
      "  0.33982939  0.02038799  0.03125441  0.01466595]\n",
      "Saving Results.\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs  = y_submission\n",
    "print(y_pred_probs[:10])\n",
    "donate_probs  = [prob for prob in y_pred_probs]\n",
    "\n",
    "print \"Saving Results.\"\n",
    "\n",
    "f = open(submission_filename, \"w\")\n",
    "\n",
    "f.write(\",Made Donation in March 2007\\n\")\n",
    "for ID, prob in zip(IDs, donate_probs):\n",
    "    f.write(\"{},{}\\n\".format(ID,prob))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
